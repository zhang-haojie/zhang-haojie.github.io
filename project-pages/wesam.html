<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WeSAM</title>
</head>


<style>
    * {
        margin: 0;
        box-sizing: border-box;
    }

    a {
        color: hsl(200.02deg 95.59% 47.48%);
        position: relative;
        text-decoration: none;
    }
    
    a:hover{
        text-decoration: underline;
    }

    hr {
        border: 0;
        border-top: 1px solid rgba(0, 0, 0, 0.1);
        height: 1px;
        margin: 0;
    }

    ul {
        padding-inline-start: calc(var(--smallFont) * 2);
    }

    table {
        border-collapse: collapse;
        margin-right: auto;
        border: 2px solid black;
        font-size: calc(var(--smallFont) * 0.8);
    }

    th, td {
        text-align: center;
        border: 1px solid gray;
        padding: calc(var(--smallFont) * 0.4);
    }

    .main {
        width: 100vw;
        min-height: 100vh;

        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
    }

    .main_head {
        background: #e9eeef;
        width: 100%;
        padding: calc(1.5 * var(--largeFont)) max(calc((100vw - 1240px) / 2), calc(1.5 * var(--largeFont)));
    }

    .main_head .title {
        line-height: 1.2;
        font-size: calc(var(--largeFont) * 1.5);
        text-align: center;
        font-weight: 500;
    }

    .main_head .conference {
        line-height: 1.2;
        font-size: var(--largeFont);
        text-align: center;
        color: #5a6268;
        font-weight: 500;
        margin: var(--smallFont) 0;
    }

    .main_head .authors {
        line-height: 1.2;
        font-size: var(--smallFont);
        text-align: center;
        font-weight: 400;
        margin: calc(var(--smallFont) * 0.8) 0;
    }

    .main_head .institute {
        line-height: 1.2;
        font-size: calc(var(--smallFont) * 0.8);
        text-align: center;
        font-weight: 400;
    }

    .main_head .btns {
        display: flex;
        flex-direction: row;
        width: 100%;
        justify-content: center;
        margin-top:  calc(var(--smallFont) * 0.8);
    }

    .main_head .btns .btn {
        padding: 8px;
        background: #6c757d;
        font-size: calc(var(--largeFont) * 0.8);
        color: #fff;
        border-radius: 8px;
        border: 1px solid gray;
        display: flex;
        align-items: center;
        cursor: pointer;
        margin: 0 10px;
    }

    .main_head .btns .btn:hover{
        box-shadow: 0px 3px 3px lightgray;
        transform: translateY(-2px);
    }

    .main_head .btns .btn span {
        line-height: 100%;
    }

    .main_content {
        width: 100%;
        padding: 0 max(calc((100vw - 1100px) / 2), calc(1.5 * var(--largeFont)));
        padding-bottom: 50px;
    }

    .main_content .section {
        margin: var(--largeFont) 0;
    }

    .main_content .section .title {
        text-align: center;
        font-size: calc(var(--largeFont) * 1.2);
        font-weight: 500;
    }

    .main_content .section hr {
        margin: var(--smallFont) 0;
    }

    .main_content .section .content {
        font-size: var(--smallFont);
        line-height: 1.5;
        text-align: justify;
    }

    .main_content .section .content img {
        width: 80%;
        margin: auto;
        display: block;
    }

    .main_content .section .content .table {
        overflow-x: auto;
        max-width: 100%;
    }


    @media screen and (max-width: 799px){
        .main_content .section .content img {
            width: 100%;
        }
    }


</style>

<body>
    <div class="main">
        <div class="main_head">
            <div class="title">
                Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation
            </div>

            <div class="conference">
                CVPR 2024
            </div>

            <hr>
            
            <div class="authors">
                <a href="https://zhang-haojie.github.io/" target="_blank">Haojie Zhang<sup>1</sup>*</a>,&nbsp;&nbsp;
                <a href="https://yysu.site" target="_blank">Yongyi Su<sup>1</sup>*</a>,&nbsp;&nbsp;
                <a href="https://alex-xun-xu.github.io/" target="_blank">Xun Xu<sup>2+</sup></a>,&nbsp;&nbsp;
                <a href="https://kuijia.site" target="_blank">Kui Jia<sup>3</sup></a>
            </div>

            <div class="institute">
                <div>
                    <sup>1</sup>South China University of Technology
                    &nbsp;&nbsp;
                    <sup>2</sup>Institute for Infocomm Research, A*STAR
                    <br>
                    <sup>3</sup>School of Data Science, The Chinese University of Hong Kong, Shenzhen
                </div>
            </div>
            
            <div class="btns">
                <div class="btn" onclick="(() => window.open(`http:\/\/arxiv.org\/abs\/2312.03502`))();">
                    <span>
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="#000000" style="width: calc(var(--largeFont))">
                            <path d="M0 0h24v24H0z" fill="none"/><path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zm-5 14H7v-2h7v2zm3-4H7v-2h10v2zm0-4H7V7h10v2z" fill="#ffffff"/>
                        </svg>
                    </span>
                    <span style="margin-left: 10px">Paper</span>
                </div>

                <div class="btn" onclick="(() => window.open(`https:\/\/github.com\/zhang-haojie\/wesam`))();">
                    <span>
                        <svg data-icon="github" fill="#000000" aria-hidden="true" viewBox="64 64 896 896" style="width: calc(var(--largeFont))">
                            <path d="M511.6 76.3C264.3 76.2 64 276.4 64 523.5 64 718.9 189.3 885 363.8 946c23.5 5.9 19.9-10.8 19.9-22.2v-77.5c-135.7 15.9-141.2-73.9-150.3-88.9C215 726 171.5 718 184.5 703c30.9-15.9 62.4 4 98.9 57.9 26.4 39.1 77.9 32.5 104 26 5.7-23.5 17.9-44.5 34.7-60.8-140.6-25.2-199.2-111-199.2-213 0-49.5 16.3-95 48.3-131.7-20.4-60.5 1.9-112.3 4.9-120 58.1-5.2 118.5 41.6 123.2 45.3 33-8.9 70.7-13.6 112.9-13.6 42.4 0 80.2 4.9 113.5 13.9 11.3-8.6 67.3-48.8 121.3-43.9 2.9 7.7 24.7 58.3 5.5 118 32.4 36.8 48.9 82.7 48.9 132.3 0 102.2-59 188.1-200 212.9a127.5 127.5 0 0138.1 91v112.5c.8 9 0 17.9 15 17.9 177.1-59.7 304.6-227 304.6-424.1 0-247.2-200.4-447.3-447.5-447.3z" fill="#ffffff">
                            </path>
                        </svg>
                    </span>
                    <span style="margin-left: 10px">Code</span>
                </div>
            </div>
        </div>

        <div class="main_content">
            <div class="section">
                <div class="title">
                    Abstract
                </div>
                <hr>
                <div class="content">
                    <img src="asserts/wesam/teaser.webp" />
                    <div>
                        The success of large language models has inspired the computer vision community to explore image segmentation foundation model that is able to zero/few-shot generalize through prompt engineering. Segment-Anything~(SAM), among others, is the state-of-the-art image segmentation foundation model demonstrating strong zero/few-shot generalization. Despite the success, recent studies reveal the weakness of SAM under strong distribution shift. In particular, SAM performs awkwardly on corrupted natural images, camouflaged images, medical images, etc. Motivated by the observations, we aim to develop a self-training based strategy to adapt SAM to target distribution. Given the unique challenges of large source dataset, high computation cost and incorrect pseudo label, we propose a weakly supervised self-training architecture with anchor regularization and low-rank finetuning to improve the robustness and computation efficiency of adaptation. We validate the effectiveness on 5 types of downstream segmentation tasks including natural clean/corrupted images, medical images, camouflaged images and robotic images. Our proposed method is task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art domain adaptation methods on almost all downstream tasks with the same testing prompt inputs.
                    </div>
                </div>
            </div>

            <div class="section">
                <div class="title">
                    Contributions & Method
                </div>
                <hr>
                <div class="content">
                    <span style="font-weight: bold; line-height: 1.5">We summarize the contributions of this work as follows:</span>
                    <ul>
                        <li>
                            We are motivated by the  generalization issue of segment-anything (SAM) model to diverse downstream segmentation tasks and propose a task-agnostic solution to adapt SAM through self-training with no access to source dataset.
                        </li>
                        <li>
                            We exploit weak supervisions, including bounding box, point-wise annotation and coarse segmentation masks, to improve the effectiveness of adaptation. The weak supervision are fully compatible with the prompt encoder of SAM.
                        </li>
                        <li>
                            Extensive experiments on 5 types of downstream instance segmentation tasks demonstrate the effectiveness of the proposed weakly supervised adaptation approach.
                        </li>
                    </ul>
    
                    <img src="asserts/wesam/pipeline.webp">
                    <p style="font-size: calc(var(--smallFont) * 0.8); color: gray; text-align: center">
                        The proposed self-training architecture with anchor network regularization and contrastive loss regularization. <span style="color: rgb(245, 95, 95)">Red arrows</span> indicates the backpropagation flow.
                    </p>
                </div>
            </div>

            <div class="section">
                <div class="title">
                    Quantitative Evaluations
                </div>
                <hr>
                <div class="content">
                    <p style="text-align: left">Table 1: Adaptation results on COCO-C dataset using bounding box prompt.</p>
                    <div class="table">
                        <table>
                            <tr><th>Method</th><th>Brit</th><th>Contr</th><th>Defoc</th><th>Elast</th><th>Fog</th><th>Frost</th><th>Gauss</th><th>Glass</th><th>Impul</th><th>Jpeg</th><th>Motn</th><th>Pixel</th><th>Shot</th><th>Snow</th><th>Zoom</th><th>Avg</th></tr>
                            <tr><th>Direct</th><td>72.83</td><td>57.34</td><td>64.47</td><td>69.36</td><td>72.39</td><td>70.50</td><td>67.20</td><td>64.43</td><td>67.65</td><td>68.23</td><td>62.72</td><td>68.60</td><td>67.44</td><td>69.02</td><td>58.80</td><td>66.73</td></tr>
                            <tr><th>TENT</th><td>76.02</td><td>61.51</td><td><strong>67.48</strong></td><td>70.88</td><td>74.89</td><td>73.88</td><td>69.01</td><td>67.10</td><td>69.28</td><td>70.25</td><td>65.45</td><td>70.81</td><td>69.96</td><td>72.37</td><td>62.59</td><td>69.43</td></tr>
                            <tr><th>SHOT</th><td>73.84</td><td>59.09</td><td>65.91</td><td>69.57</td><td>73.98</td><td>72.51</td><td>68.30</td><td>66.09</td><td>68.61</td><td>69.45</td><td>64.56</td><td>70.48</td><td>68.77</td><td>71.03</td><td>60.17</td><td>68.16</td></tr>
                            <tr><th>Soft Teacher</th><td>73.90</td><td><strong>62.12</strong></td><td>65.41</td><td>71.32</td><td>72.16</td><td>73.27</td><td>68.84</td><td>67.49</td><td>68.73</td><td>70.18</td><td><strong>66.88</strong></td><td>69.79</td><td>70.08</td><td>73.33</td><td>64.88</td><td>69.23</td></tr>
                            <tr><th>TRIBE</th><td>76.40</td><td>60.86</td><td>66.19</td><td>72.72</td><td>75.08</td><td>75.14</td><td>70.34</td><td>66.66</td><td>70.83</td><td>72.42</td><td>65.94</td><td>70.24</td><td><strong>70.66</strong></td><td>74.22</td><td>64.56</td><td>70.15</td></tr>
                            <tr><th>DePT</th><td>69.15</td><td>57.26</td><td>59.08</td><td>66.80</td><td>58.73</td><td>66.75</td><td>66.78</td><td>62.74</td><td>65.65</td><td>66.39</td><td>61.66</td><td>66.65</td><td>67.57</td><td>66.62</td><td>58.21</td><td>64.42</td></tr>
                            <tr><th>WDASS</th><td>76.21</td><td>60.57</td><td>67.07</td><td>72.34</td><td>75.97</td><td>74.63</td><td>69.84</td><td>67.88</td><td>69.92</td><td>71.36</td><td>66.25</td><td><strong>71.99</strong></td><td>70.32</td><td>72.25</td><td>63.61</td><td>70.01</td></tr>
                            <tr><th>OURS</th><td><strong>78.50</strong></td><td>61.05</td><td>66.99</td><td><strong>73.93</strong></td><td><strong>77.09</strong></td><td><strong>76.10</strong></td><td><strong>72.02</strong></td><td><strong>68.21</strong></td><td><strong>71.29</strong></td><td><strong>72.77</strong></td><td>66.33</td><td>70.90</td><td>70.28</td><td><strong>75.07</strong></td><td><strong>65.33</strong></td><td><strong>71.05</strong></td></tr>
                            <tr><th>Supervised</th><td>78.86</td><td>74.81</td><td>72.04</td><td>74.32</td><td>78.01</td><td>77.14</td><td>73.43</td><td>72.12</td><td>74.08</td><td>75.30</td><td>71.39</td><td>75.15</td><td>74.25</td><td>76.34</td><td>68.04</td><td>74.35</td></tr>                    
                        </table>
                    </div>

                    <br>

                    <p style="text-align: left">Table 2: Adaptation results on natural clean image datasets.</p>
                    <div class="table">
                        <table>
                            <tr><th rowspan="2">Method</th><th colspan="3">COCO 2017</th><th colspan="3">Pascal VOC</th></tr>
                            <tr><th>box</th><th>point</th><th>poly</th><th>box</th><th>point</th><th>poly</th></tr>
                            <tr><th>Direct</th><td>74.29</td><td>55.06</td><td>65.64</td><td>69.21</td><td>69.21</td><td>60.79</td></tr>
                            <tr><th>TENT</th><td>78.21</td><td>52.99</td><td>71.51</td><td>80.24</td><td>74.97</td><td>65.03</td></tr>
                            <tr><th>SHOT</th><td>75.18</td><td>58.46</td><td>69.26</td><td>79.80</td><td>74.26</td><td>63.38</td></tr>
                            <tr><th>Soft Teacher</th><td>75.94</td><td>43.36</td><td>68.27</td><td>72.93</td><td>56.09</td><td>62.20</td></tr>
                            <tr><th>TRIBE</th><td>77.56</td><td>49.56</td><td>70.99</td><td>78.87</td><td>69.21</td><td>65.39</td></tr>
                            <tr><th>DePT</th><td>71.00</td><td>37.35</td><td>63.27</td><td>74.09</td><td>42.99</td><td>59.94</td></tr>
                            <tr><th>WDASS</th><td>77.29</td><td>60.55</td><td>70.19</td><td>80.12</td><td><strong>76.15</strong></td><td><strong>66.98</strong></td></tr>
                            <tr><th>OURS</th><td><strong>80.12</strong></td><td><strong>62.09</strong></td><td><strong>72.33</strong></td><td><strong>80.27</strong></td><td>74.15</td><td>66.72</td></tr>
                            <tr><th>Supervised</th><td>81.50</td><td>69.77</td><td>73.39</td><td>81.23</td><td>76.98</td><td>71.32</td></tr>
                        </table>
                    </div>

                    <br>

                    <p style="text-align: left">Table 3: Adaptation results on medical image segmentation datasets.</p>
                    <div class="table">
                        <table>
                            <tr><th rowspan="2">Method</th><th colspan="3">kvasir-SEG</th><th colspan="3">ISIC</th></tr>
                            <tr><th>box</th><th>point</th><th>poly</th><th>box</th><th>point</th><th>poly</th></tr>
                            <tr><th>Direct</th><td>81.59</td><td>62.30</td><td>54.03</td><td>66.74</td><td>53.42</td><td>62.82</td></tr>
                            <tr><th>TENT</th><td>82.47</td><td>61.84</td><td>62.97</td><td>71.76</td><td>53.46</td><td>67.12</td></tr>
                            <tr><th>SHOT</th><td>82.30</td><td>63.76</td><td>61.34</td><td>71.99</td><td>55.99</td><td>66.86</td></tr>
                            <tr><th>Soft Teacher</th><td>84.12</td><td>73.53</td><td>58.15</td><td>75.74</td><td>54.95</td><td>72.29</td></tr>
                            <tr><th>TRIBE</th><td>85.05</td><td>73.03</td><td>64.61</td><td>72.61</td><td>50.36</td><td>67.99</td></tr>
                            <tr><th>DePT</th><td>81.91</td><td>52.06</td><td>61.55</td><td>78.43</td><td>46.79</td><td>72.75</td></tr>
                            <tr><th>WDASS</th><td>84.01</td><td>63.78</td><td>64.78</td><td>74.23</td><td>55.63</td><td>67.84</td></tr>
                            <tr><th>OURS</th><td><strong>85.47</strong></td><td><strong>75.23</strong></td><td><strong>67.40</strong></td><td><strong>80.01</strong></td><td><strong>62.12</strong></td><td><strong>75.36</strong></td></tr>
                            <tr><th>Supervised</th><td>85.89</td><td>77.54</td><td>81.64</td><td>81.62</td><td>79.81</td><td>80.26</td></tr>                    
                        </table>
                    </div>

                    <br>

                    <p style="text-align: left">Table 4: Adaptation results on camouflaged object datasets.</p>
                    <div class="table">
                        <table>
                            <tr><th rowspan="2">Method</th><th colspan="3">CHAMELEON</th><th colspan="3">CAMO</th><th colspan="3">COD10K</th></tr>
                            <tr><th>box</th><th>point</th><th>poly</th><th>box</th><th>point</th><th>poly</th><th>box</th><th>point</th><th>poly</th></tr>
                            <tr><th>Direct</th><td>51.32</td><td>39.37</td><td>45.78</td><td>62.72</td><td>57.43</td><td>50.85</td><td>66.32</td><td>63.61</td><td>40.04</td></tr>
                            <tr><th>TENT</th><td>65.48</td><td>54.53</td><td>53.06</td><td>71.24</td><td>59.59</td><td>60.29</td><td>69.36</td><td>61.94</td><td>43.36</td></tr>
                            <tr><th>SHOT</th><td>68.60</td><td>62.47</td><td>54.36</td><td>71.61</td><td>62.78</td><td>58.72</td><td>69.09</td><td>65.25</td><td>42.38</td></tr>
                            <tr><th>Soft Teacher</th><td>65.92</td><td>44.17</td><td>46.72</td><td>62.30</td><td>48.64</td><td>51.26</td><td>66.32</td><td>50.04</td><td>32.27</td></tr>
                            <tr><th>TRIBE</th><td>71.00</td><td>52.80</td><td>54.99</td><td>66.00</td><td>61.97</td><td>60.54</td><td>67.84</td><td>63.62</td><td>42.75</td></tr>
                            <tr><th>DePT</th><td>54.48</td><td>33.46</td><td>42.47</td><td>55.44</td><td>33.07</td><td>48.63</td><td>59.32</td><td>34.06</td><td>35.51</td></tr>
                            <tr><th>WDASS</th><td>71.91</td><td>62.40</td><td>56.80</td><td>71.25</td><td>63.39</td><td>62.29</td><td>71.42</td><td>65.61</td><td>43.93</td></tr>
                            <tr><th>OURS</th><td><strong>75.94</strong></td><td><strong>74.00</strong></td><td><strong>66.83</strong></td><td><strong>73.42</strong></td><td><strong>65.55</strong></td><td><strong>62.90</strong></td><td><strong>71.93</strong></td><td><strong>70.55</strong></td><td><strong>45.87</strong></td></tr>
                            <tr><th>Supervised</th><td>78.05</td><td>85.86</td><td>68.38</td><td>79.17</td><td>77.01</td><td>67.12</td><td>78.06</td><td>78.44</td><td>64.90</td></tr>
                        </table>
                    </div>

                    <br>

                    <p style="text-align: left">Table 5: Adaptation results on robotic image datasets.</p>
                    <div class="table">
                        <table>
                            <tr><th rowspan="2">Method</th><th colspan="3">OCID</th><th colspan="3">OSD</th></tr>
                            <tr><th>box</th><th>point</th><th>poly</th><th>box</th><th>point</th><th>poly</th></tr>
                            <tr><th>Direct</th><td>86.35</td><td>71.41</td><td>72.81</td><td>87.62</td><td>78.86</td><td>80.77</td></tr>
                            <tr><th>TENT</th><td>87.77</td><td>66.61</td><td><strong>77.53</strong></td><td>88.10</td><td>80.53</td><td>87.85</td></tr>
                            <tr><th>SHOT</th><td>88.06</td><td>74.39</td><td>76.25</td><td>88.09</td><td>80.52</td><td>87.86</td></tr>
                            <tr><th>Soft Teacher</th><td>84.98</td><td>68.46</td><td>73.75</td><td>90.41</td><td>80.49</td><td>87.00</td></tr>
                            <tr><th>TRIBE</th><td>86.77</td><td>67.86</td><td>76.50</td><td>90.42</td><td><strong>80.54</strong></td><td>87.84</td></tr>
                            <tr><th>DePT</th><td>82.00</td><td>56.52</td><td>70.92</td><td>81.84</td><td>69.06</td><td>82.50</td></tr>
                            <tr><th>WDASS</th><td>87.68</td><td>77.13</td><td>76.70</td><td>88.07</td><td>80.52</td><td>88.19</td></tr>
                            <tr><th>OURS</th><td><strong>88.09</strong></td><td><strong>80.14</strong></td><td>77.41</td><td><strong>92.11</strong></td><td>80.51</td><td><strong>89.72</strong></td></tr>
                            <tr><th>Supervised</th><td>91.24</td><td>89.22</td><td>79.23</td><td>92.14</td><td>82.41</td><td>90.83</td></tr>
                        </table>
                    </div>
                </div>
            </div>

            <div class="section">
                <div class="title">
                    Qualitative Evaluations
                </div>
                <hr>
                <div class="content">
                        <img width="100%" src="asserts/wesam/COCO_vis.webp" />
                    <br>
                        <img width="100%" src="asserts/wesam/corrupt-1.webp" />
                    <br>
                        <img width="100%" src="asserts/wesam/corrupt-2.webp" />
                    <br>
                        <img width="100%" src="asserts/wesam/CAMO_vis.webp" />
                    <br>
                        <img width="100%" src="asserts/wesam/ISIC_vis.webp" />
                    <br>
                        <img width="100%" src="asserts/wesam/OCID_vis.webp" />
                </div>
            </div>
            

            <div class="section">
                <div class="title">
                    Citation
                </div>
                <hr>
                <div class="content">
                    <pre data-v-66da7b8f="" style="overflow: auto; background: rgb(230, 230, 230); border-radius: 10px;"><code>  
    @article{zhang2023improving,
        title={Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation},
        author={Zhang, Haojie and Su, Yongyi and Xu, Xun and Jia, Kui},
        journal={arXiv preprint arXiv:2312.03502},
        year={2023}
    }
                </code></pre>
                </div>
            </div>

        </div>
    </div>
</body>


<script>
    addEventListener("DOMContentLoaded", e => {
        let new_val = document.body.clientWidth;
        if (new_val < 800){
            new_val = new_val / 0.6;
        } else if (new_val > 1240){
            new_val = 1240;
        }

        const smallFont = new_val * 0.015 + 'px';
        const largeFont = new_val * 0.02 + 'px';

        const main_div = document.getElementsByClassName('main')[0];
        main_div.style='--smallFont: ' + smallFont + '; --largeFont: ' + largeFont;
    })

    addEventListener("resize", e => {
        let new_val = document.body.clientWidth;
        if (new_val < 800){
            new_val = new_val / 0.6;
        } else if (new_val > 1240){
            new_val = 1240;
        }

        const smallFont = new_val * 0.015 + 'px';
        const largeFont = new_val * 0.02 + 'px';

        const main_div = document.getElementsByClassName('main')[0];
        main_div.style='--smallFont: ' + smallFont + '; --largeFont: ' + largeFont;
    });

</script>
</html>